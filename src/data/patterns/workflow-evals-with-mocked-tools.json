{
  "id": "workflow-evals-with-mocked-tools",
  "title": "Workflow Evals with Mocked Tools",
  "title_ko": "모의 도구를 사용한 워크플로우 평가",
  "category": "Reliability & Eval",
  "status": "emerging",
  "original_url": "https://lethain.com/agents-evals/",
  "problem": {
    "en": "Unit tests and linters validate individual components but don't test agent workflows end-to-end. It's easy to create prompts that don't work well despite all underlying pieces being correct. Need to validate that prompts and tools work together as a system.",
    "ko": "단위 테스트와 린터는 개별 구성 요소를 검증하지만 에이전트 워크플로우를 엔드투엔드로 테스트하지 않습니다. 모든 기본 조각이 올바르더라도 잘 작동하지 않는 프롬프트를 만들기 쉽습니다. 프롬프트와 도구가 시스템으로 함께 작동하는지 검증해야 합니다."
  },
  "solution": {
    "en": "Implement workflow evals (simulations) that test complete agent workflows with mocked tools. Every tool has true and mock versions. Evaluate with objective criteria (which tools called) and subjective criteria (agent-as-judge). Integrate with CI/CD.",
    "ko": "모의 도구로 완전한 에이전트 워크플로우를 테스트하는 워크플로우 평가(시뮬레이션)를 구현합니다. 모든 도구에는 실제 버전과 모의 버전이 있습니다. 객관적 기준(호출된 도구)과 주관적 기준(에이전트를 판사로)으로 평가합니다. CI/CD와 통합합니다."
  },
  "ascii_diagram": "┌─────────────────────────────────┐\n│       Eval Configuration        │\n├─────────────────────────────────┤\n│ • Initial prompt                │\n│ • Metadata/context              │\n│ • Expected tools                │\n│ • Forbidden tools               │\n│ • Subjective criteria           │\n└───────────────┬─────────────────┘\n                │\n┌───────────────▼─────────────────┐\n│    Swap in Mock Tools           │\n└───────────────┬─────────────────┘\n                │\n┌───────────────▼─────────────────┐\n│    Run Agent Workflow           │\n└───────────────┬─────────────────┘\n                │\n        ┌───────┴───────┐\n        │               │\n┌───────▼───────┐ ┌─────▼─────────┐\n│ Objective     │ │ Subjective    │\n│ Criteria      │ │ Criteria      │\n│ (tool calls)  │ │ (LLM judge)   │\n└───────┬───────┘ └───────┬───────┘\n        └───────┬─────────┘\n                │\n        ┌───────▼───────┐\n        │  Pass/Fail    │\n        │  + Details    │\n        └───────────────┘",
  "mermaid_diagram": "flowchart TD\n    A[Eval Config] --> B[Load Mock Tools]\n    B --> C[Run Agent with Prompt]\n    C --> D[Track Tool Calls]\n    D --> E{Objective Check}\n    E -->|Expected tools called?| F[Check Subjective]\n    E -->|Forbidden tools called?| G[Fail]\n    F --> H[Agent-as-Judge]\n    H --> I{Pass?}\n    I -->|Yes| J[Report Success]\n    I -->|No| K[Report Failure]\n    J --> L[CI/CD Integration]\n    K --> L",
  "code_example": "# Eval configuration\nevals = [{\n    'name': 'slack_jira_workflow',\n    'prompt': 'Add reaction to JIRA ticket in this message',\n    'expected_tools': ['slack_get_message', 'jira_get_ticket', 'slack_add_reaction'],\n    'forbidden_tools': ['slack_send_message'],\n    'subjective_criteria': 'Response was helpful and accurate'\n}]\n\n# Mock tool registry\nclass MockToolRegistry:\n    def get_tool(self, name):\n        return self.mocks.get(name, self.real_tools[name])\n\n# Run eval\ndef run_eval(config):\n    result = agent.run(prompt=config['prompt'], tools=mock_registry)\n    \n    # Objective: Check tool usage\n    passed = all(t in result.tools_used for t in config['expected_tools'])\n    passed &= all(t not in result.tools_used for t in config['forbidden_tools'])\n    \n    # Subjective: Agent as judge\n    if passed:\n        passed = llm_judge(result.response, config['subjective_criteria'])\n    \n    return {'passed': passed, 'details': result}",
  "when_to_use": {
    "en": ["Agent workflows with tool side effects", "CI/CD pipeline validation", "Prompt engineering and optimization", "Regression testing for agent changes"],
    "ko": ["도구 부작용이 있는 에이전트 워크플로우", "CI/CD 파이프라인 검증", "프롬프트 엔지니어링 및 최적화", "에이전트 변경에 대한 회귀 테스트"]
  },
  "pros": {
    "en": ["End-to-end validation of prompts + tools", "Fast feedback before production", "Safe testing with mocked side effects", "Both objective and subjective criteria"],
    "ko": ["프롬프트 + 도구의 엔드투엔드 검증", "프로덕션 전 빠른 피드백", "모의 부작용으로 안전한 테스트", "객관적 및 주관적 기준 모두"]
  },
  "cons": {
    "en": ["LLM non-determinism causes flaky tests", "Mock maintenance overhead", "Prompt-driven workflows more flaky", "Mixed results provide ambiguous signal"],
    "ko": ["LLM 비결정성으로 불안정한 테스트", "모의 유지보수 오버헤드", "프롬프트 기반 워크플로우 더 불안정", "혼합 결과는 모호한 신호 제공"]
  },
  "tags": ["evals", "testing", "ci-cd", "mocked-tools", "simulations", "workflow-validation"]
}
