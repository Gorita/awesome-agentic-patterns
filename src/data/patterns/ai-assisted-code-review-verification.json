{
  "id": "ai-assisted-code-review-verification",
  "title": "AI-Assisted Code Review / Verification",
  "title_ko": "AI 지원 코드 리뷰 / 검증",
  "category": "Feedback Loops",
  "status": "emerging",
  "original_url": "https://www.youtube.com/watch?v=BGgsoIgbT_Y",
  "problem": {
    "en": "As AI generates increasing amounts of code, the bottleneck shifts from generation to verification. Ensuring AI-generated code is semantically correct, aligns with intent (especially if underspecified), and meets quality standards becomes crucial and time-consuming.",
    "ko": "AI가 점점 더 많은 코드를 생성함에 따라 병목 현상이 생성에서 검증으로 이동합니다. AI 생성 코드가 의미적으로 올바르고, 의도(특히 불충분하게 지정된 경우)에 부합하며, 품질 표준을 충족하는지 확인하는 것이 중요하고 시간이 많이 걸립니다."
  },
  "solution": {
    "en": "Employ AI-powered tools to assist code review: analyze changes and highlight potential issues, summarize intent/impact of changes, interactive systems for explaining code or justifying decisions, ensure alignment with user's 'mind's eye' intent.",
    "ko": "AI 기반 도구를 사용하여 코드 리뷰 지원: 변경 사항 분석 및 잠재적 문제 강조, 변경의 의도/영향 요약, 코드 설명 또는 결정 정당화를 위한 대화형 시스템, 사용자의 '마음의 눈' 의도와의 일치 보장."
  },
  "ascii_diagram": "┌─────────────────────────────────┐\n│   AI-Generated Code Changes     │\n└───────────────┬─────────────────┘\n                │\n┌───────────────▼─────────────────┐\n│    AI Review Assistant          │\n├─────────────────────────────────┤\n│ • Analyze changes               │\n│ • Highlight potential issues    │\n│ • Summarize intent/impact       │\n│ • Explain code decisions        │\n└───────────────┬─────────────────┘\n                │\n┌───────────────▼─────────────────┐\n│      Human Reviewer             │\n│  Focus on high-level intent     │\n│  and business logic             │\n└───────────────┬─────────────────┘\n                │\n┌───────────────▼─────────────────┐\n│   Confident Codebase            │\n└─────────────────────────────────┘",
  "mermaid_diagram": "flowchart TD\n    A[AI-Generated Code] --> B[AI Review Assistant]\n    B --> C[Analyze Changes]\n    B --> D[Highlight Issues]\n    B --> E[Summarize Intent]\n    C --> F[Human Reviewer]\n    D --> F\n    E --> F\n    F --> G{Aligns with Intent?}\n    G -->|Yes| H[Approve]\n    G -->|No| I[Request Changes]",
  "code_example": "# AI-Assisted Code Review Integration\n\nclass AICodeReviewAssistant:\n    def review_changes(self, diff):\n        analysis = {\n            'issues': self.detect_potential_issues(diff),\n            'intent_summary': self.summarize_intent(diff),\n            'impact_analysis': self.analyze_impact(diff),\n            'questions': self.generate_review_questions(diff)\n        }\n        return analysis\n    \n    def explain_decision(self, code_section, question):\n        # Interactive Q&A about code decisions\n        return self.explain(code_section, question)\n\n# Integration into PR review process\ndef review_pr(pr):\n    ai_review = assistant.review_changes(pr.diff)\n    \n    # Human focuses on alignment with intent\n    print(f\"Intent Summary: {ai_review['intent_summary']}\")\n    print(f\"Potential Issues: {ai_review['issues']}\")\n    \n    # Human can ask clarifying questions\n    explanation = assistant.explain_decision(\n        pr.diff, \"Why was this approach chosen?\"\n    )",
  "when_to_use": {
    "en": ["Reviewing large AI-generated changes", "Verifying alignment with underspecified intent", "PR review process integration", "Building confidence in AI-assisted codebases"],
    "ko": ["대규모 AI 생성 변경 검토", "불충분하게 지정된 의도와의 일치 검증", "PR 리뷰 프로세스 통합", "AI 지원 코드베이스에 대한 신뢰 구축"]
  },
  "pros": {
    "en": ["Makes review more efficient", "Highlights issues humans might miss", "Summarizes complex changes", "Interactive explanation capability"],
    "ko": ["리뷰를 더 효율적으로", "인간이 놓칠 수 있는 문제 강조", "복잡한 변경 요약", "대화형 설명 기능"]
  },
  "cons": {
    "en": ["AI may miss subtle bugs", "Over-reliance risk", "Explanation quality varies", "Adds tooling complexity"],
    "ko": ["AI가 미묘한 버그를 놓칠 수 있음", "과도한 의존 위험", "설명 품질 차이", "도구 복잡성 추가"]
  },
  "tags": ["code-review", "verification", "quality-assurance", "human-ai-collaboration", "trust", "explainability"]
}
